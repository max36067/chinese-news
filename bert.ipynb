{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGsv1QjcapvyWZ9A1sltry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/max36067/chinese-news/blob/master/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrMKNgnS57q7",
        "colab_type": "code",
        "outputId": "c45de7f7-60ad-43a9-98dd-3dccd85ef79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a_vtVQM8b07",
        "colab_type": "code",
        "outputId": "930bd015-eaf7-430f-8b4c-45f66328239d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import glob\n",
        "fn = glob.glob(\"/content/drive/My Drive/chinese/*\")\n",
        "fn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/chinese/bert_model.ckpt.index',\n",
              " '/content/drive/My Drive/chinese/bert_config.json',\n",
              " '/content/drive/My Drive/chinese/bert_model.ckpt.meta',\n",
              " '/content/drive/My Drive/chinese/vocab.txt',\n",
              " '/content/drive/My Drive/chinese/bert_model.ckpt.data-00000-of-00001',\n",
              " '/content/drive/My Drive/chinese/content.csv',\n",
              " '/content/drive/My Drive/chinese/news_content.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz3gRdlRA_uQ",
        "colab_type": "code",
        "outputId": "109d6db0-3b48-4875-c3b6-ca04e4b26fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(fn[6], encoding=\"utf-8\")\n",
        "train_df = train_df.dropna().drop([0])\n",
        "train_df.columns = [\"content\", \"label\"]\n",
        "train_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>鴻海 (2317-TW) 與日本系統晶片 (SoC) 業者索思未來 (Socionext)、...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>秋冬旅遊補助政策下激勵飯店業者業績表現，業者公布的去年 12 月合併營收來看，陸續繳出亮眼成...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>材料 - KY(4763-TW) 擴產效益顯現，去年第 4 季營收 6.07 億元創新高， ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>美國消費性電子展 (CES) 北美時間 7 日正式開展，台廠除聯發科 (2454-TW) 發...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>美系外資指出，LED 廠晶電 (2448-TW) 今年 Mini LED 專案持續增加，負責...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>水處理環保設備及技術服務商基士德 - KY (6641-TW) 2019 年 11 月營收以...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>軸承廠兆利 (3548-TW) 今年積極搶灘折疊手機、雙螢幕筆電等新應用領域，全年業績成長估...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>藥華藥 (6446-TW) 旗下創新生物藥百斯瑞明 (Besremi) 去年初取得歐盟核准後...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>看好伺服器產業回溫，加上市場傳微軟取得美國國防部雲端合約，有助台廠供應鏈營運，外資看好緯創 ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>太陽能電池廠太極 (4934-TW) 今年單晶年產能上看 500 MW，訂單能見度已看至第 ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>184 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               content label\n",
              "1    鴻海 (2317-TW) 與日本系統晶片 (SoC) 業者索思未來 (Socionext)、...     2\n",
              "2    秋冬旅遊補助政策下激勵飯店業者業績表現，業者公布的去年 12 月合併營收來看，陸續繳出亮眼成...     2\n",
              "3    材料 - KY(4763-TW) 擴產效益顯現，去年第 4 季營收 6.07 億元創新高， ...     2\n",
              "4    美國消費性電子展 (CES) 北美時間 7 日正式開展，台廠除聯發科 (2454-TW) 發...     2\n",
              "5    美系外資指出，LED 廠晶電 (2448-TW) 今年 Mini LED 專案持續增加，負責...     2\n",
              "..                                                 ...   ...\n",
              "199  水處理環保設備及技術服務商基士德 - KY (6641-TW) 2019 年 11 月營收以...     2\n",
              "200  軸承廠兆利 (3548-TW) 今年積極搶灘折疊手機、雙螢幕筆電等新應用領域，全年業績成長估...     2\n",
              "201  藥華藥 (6446-TW) 旗下創新生物藥百斯瑞明 (Besremi) 去年初取得歐盟核准後...     2\n",
              "203  看好伺服器產業回溫，加上市場傳微軟取得美國國防部雲端合約，有助台廠供應鏈營運，外資看好緯創 ...     2\n",
              "204  太陽能電池廠太極 (4934-TW) 今年單晶年產能上看 500 MW，訂單能見度已看至第 ...     2\n",
              "\n",
              "[184 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tV8YtIDB5k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "token_dict = {}\n",
        "with codecs.open(fn[3], \"r\", \"utf8\") as reader:\n",
        "  for line in reader:\n",
        "      token = line.strip()\n",
        "      token_dict[token] = len(token_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lObVjRiAD09F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trans = {\"2\": '正面',\n",
        "         '1': '中立',\n",
        "         '0': '負面'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX8m77EeDMS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert4keras.tokenizer import Tokenizer\n",
        "from bert4keras.snippets import sequence_padding\n",
        "import numpy as np\n",
        "# 如果要用fit_generator的話，需要使用yield將參數先傳入記憶體\n",
        "tokenize = Tokenizer(token_dict, do_lower_case=True)\n",
        "def data_generator(x, y, batch=32):\n",
        "  while True:\n",
        "    count = 0\n",
        "    idx = np.arange(len(x))\n",
        "    # 把資料打亂才不會一次看同樣的東西\n",
        "    np.random.shuffle(idx)\n",
        "    indices, labels = [], []\n",
        "    for l in idx:\n",
        "      # 把字tokenize output會出現字的索引+區別1, 2段的token\n",
        "      ids, segments = tokenize.encode(first_text=x[l], max_length=256)\n",
        "      indices.append(ids)\n",
        "      labels.append(y[l])\n",
        "      count += 1\n",
        "      # 因為fit_generator不會有區別x, y值，需要將兩者一次傳入\n",
        "      if count == batch or l == idx[-1]:\n",
        "        indices = sequence_padding(indices)\n",
        "        labels = sequence_padding(labels)\n",
        "        yield [np.array(indices), np.zeros_like(indices)], np.array(labels)\n",
        "        indices, labels = [], []\n",
        "        count = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSHc9loWIGqI",
        "colab_type": "code",
        "outputId": "5a6f2de5-3bcd-4167-abff-5ca8c9eda638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "batch = 16\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_df['content'], train_df['label'], test_size=0.1)\n",
        "x_train = x_train.values\n",
        "x_test = x_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "train = data_generator(x_train, y_train, batch)\n",
        "test = data_generator(x_test, y_test, batch=4)\n",
        "x = print(test)\n",
        "y = next(test)[1]\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object data_generator at 0x7fcd8ac019e8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFEBlcgbO7Qd",
        "colab_type": "code",
        "outputId": "3ea35de2-e312-4f85-b40d-98e1d5c233ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from bert4keras.bert import build_bert_model\n",
        "from bert4keras.backend import set_gelu\n",
        "from bert4keras.optimizers import Adam\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "\n",
        "set_gelu(\"tanh\")\n",
        "checkpoint_path = fn[4].split('.')[0] + '.ckpt'\n",
        "bert_model = build_bert_model(\n",
        "    fn[1],\n",
        "    checkpoint_path,\n",
        "    return_keras_model=False,\n",
        "    with_pool=True\n",
        ")\n",
        "# bert後不用接太多層，本身就有不錯的預測值\n",
        "x = Dropout(0.1)(bert_model.model.output)\n",
        "x = Dense(3, activation=\"softmax\", kernel_initializer=bert_model.initializer)(x)\n",
        "model = Model(bert_model.model.input, x)\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=Adam(2e-5),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> searching: bert/embeddings/word_embeddings, found name: bert/embeddings/word_embeddings\n",
            "==> searching: bert/embeddings/token_type_embeddings, found name: bert/embeddings/token_type_embeddings\n",
            "==> searching: bert/embeddings/position_embeddings, found name: bert/embeddings/position_embeddings\n",
            "==> searching: bert/embeddings/LayerNorm/gamma, found name: bert/embeddings/LayerNorm/gamma\n",
            "==> searching: bert/embeddings/LayerNorm/beta, found name: bert/embeddings/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_0/attention/self/query/kernel, found name: bert/encoder/layer_0/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_0/attention/self/query/bias, found name: bert/encoder/layer_0/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_0/attention/self/key/kernel, found name: bert/encoder/layer_0/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_0/attention/self/key/bias, found name: bert/encoder/layer_0/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_0/attention/self/value/kernel, found name: bert/encoder/layer_0/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_0/attention/self/value/bias, found name: bert/encoder/layer_0/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_0/attention/output/dense/kernel, found name: bert/encoder/layer_0/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_0/attention/output/dense/bias, found name: bert/encoder/layer_0/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_0/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_0/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_0/attention/output/LayerNorm/beta, found name: bert/encoder/layer_0/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_0/intermediate/dense/kernel, found name: bert/encoder/layer_0/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_0/intermediate/dense/bias, found name: bert/encoder/layer_0/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_0/output/dense/kernel, found name: bert/encoder/layer_0/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_0/output/dense/bias, found name: bert/encoder/layer_0/output/dense/bias\n",
            "==> searching: bert/encoder/layer_0/output/LayerNorm/gamma, found name: bert/encoder/layer_0/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_0/output/LayerNorm/beta, found name: bert/encoder/layer_0/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_1/attention/self/query/kernel, found name: bert/encoder/layer_1/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_1/attention/self/query/bias, found name: bert/encoder/layer_1/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_1/attention/self/key/kernel, found name: bert/encoder/layer_1/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_1/attention/self/key/bias, found name: bert/encoder/layer_1/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_1/attention/self/value/kernel, found name: bert/encoder/layer_1/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_1/attention/self/value/bias, found name: bert/encoder/layer_1/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_1/attention/output/dense/kernel, found name: bert/encoder/layer_1/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_1/attention/output/dense/bias, found name: bert/encoder/layer_1/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_1/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_1/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_1/attention/output/LayerNorm/beta, found name: bert/encoder/layer_1/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_1/intermediate/dense/kernel, found name: bert/encoder/layer_1/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_1/intermediate/dense/bias, found name: bert/encoder/layer_1/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_1/output/dense/kernel, found name: bert/encoder/layer_1/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_1/output/dense/bias, found name: bert/encoder/layer_1/output/dense/bias\n",
            "==> searching: bert/encoder/layer_1/output/LayerNorm/gamma, found name: bert/encoder/layer_1/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_1/output/LayerNorm/beta, found name: bert/encoder/layer_1/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_2/attention/self/query/kernel, found name: bert/encoder/layer_2/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_2/attention/self/query/bias, found name: bert/encoder/layer_2/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_2/attention/self/key/kernel, found name: bert/encoder/layer_2/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_2/attention/self/key/bias, found name: bert/encoder/layer_2/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_2/attention/self/value/kernel, found name: bert/encoder/layer_2/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_2/attention/self/value/bias, found name: bert/encoder/layer_2/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_2/attention/output/dense/kernel, found name: bert/encoder/layer_2/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_2/attention/output/dense/bias, found name: bert/encoder/layer_2/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_2/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_2/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_2/attention/output/LayerNorm/beta, found name: bert/encoder/layer_2/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_2/intermediate/dense/kernel, found name: bert/encoder/layer_2/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_2/intermediate/dense/bias, found name: bert/encoder/layer_2/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_2/output/dense/kernel, found name: bert/encoder/layer_2/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_2/output/dense/bias, found name: bert/encoder/layer_2/output/dense/bias\n",
            "==> searching: bert/encoder/layer_2/output/LayerNorm/gamma, found name: bert/encoder/layer_2/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_2/output/LayerNorm/beta, found name: bert/encoder/layer_2/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_3/attention/self/query/kernel, found name: bert/encoder/layer_3/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_3/attention/self/query/bias, found name: bert/encoder/layer_3/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_3/attention/self/key/kernel, found name: bert/encoder/layer_3/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_3/attention/self/key/bias, found name: bert/encoder/layer_3/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_3/attention/self/value/kernel, found name: bert/encoder/layer_3/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_3/attention/self/value/bias, found name: bert/encoder/layer_3/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_3/attention/output/dense/kernel, found name: bert/encoder/layer_3/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_3/attention/output/dense/bias, found name: bert/encoder/layer_3/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_3/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_3/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_3/attention/output/LayerNorm/beta, found name: bert/encoder/layer_3/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_3/intermediate/dense/kernel, found name: bert/encoder/layer_3/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_3/intermediate/dense/bias, found name: bert/encoder/layer_3/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_3/output/dense/kernel, found name: bert/encoder/layer_3/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_3/output/dense/bias, found name: bert/encoder/layer_3/output/dense/bias\n",
            "==> searching: bert/encoder/layer_3/output/LayerNorm/gamma, found name: bert/encoder/layer_3/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_3/output/LayerNorm/beta, found name: bert/encoder/layer_3/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_4/attention/self/query/kernel, found name: bert/encoder/layer_4/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_4/attention/self/query/bias, found name: bert/encoder/layer_4/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_4/attention/self/key/kernel, found name: bert/encoder/layer_4/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_4/attention/self/key/bias, found name: bert/encoder/layer_4/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_4/attention/self/value/kernel, found name: bert/encoder/layer_4/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_4/attention/self/value/bias, found name: bert/encoder/layer_4/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_4/attention/output/dense/kernel, found name: bert/encoder/layer_4/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_4/attention/output/dense/bias, found name: bert/encoder/layer_4/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_4/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_4/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_4/attention/output/LayerNorm/beta, found name: bert/encoder/layer_4/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_4/intermediate/dense/kernel, found name: bert/encoder/layer_4/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_4/intermediate/dense/bias, found name: bert/encoder/layer_4/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_4/output/dense/kernel, found name: bert/encoder/layer_4/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_4/output/dense/bias, found name: bert/encoder/layer_4/output/dense/bias\n",
            "==> searching: bert/encoder/layer_4/output/LayerNorm/gamma, found name: bert/encoder/layer_4/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_4/output/LayerNorm/beta, found name: bert/encoder/layer_4/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_5/attention/self/query/kernel, found name: bert/encoder/layer_5/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_5/attention/self/query/bias, found name: bert/encoder/layer_5/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_5/attention/self/key/kernel, found name: bert/encoder/layer_5/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_5/attention/self/key/bias, found name: bert/encoder/layer_5/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_5/attention/self/value/kernel, found name: bert/encoder/layer_5/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_5/attention/self/value/bias, found name: bert/encoder/layer_5/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_5/attention/output/dense/kernel, found name: bert/encoder/layer_5/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_5/attention/output/dense/bias, found name: bert/encoder/layer_5/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_5/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_5/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_5/attention/output/LayerNorm/beta, found name: bert/encoder/layer_5/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_5/intermediate/dense/kernel, found name: bert/encoder/layer_5/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_5/intermediate/dense/bias, found name: bert/encoder/layer_5/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_5/output/dense/kernel, found name: bert/encoder/layer_5/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_5/output/dense/bias, found name: bert/encoder/layer_5/output/dense/bias\n",
            "==> searching: bert/encoder/layer_5/output/LayerNorm/gamma, found name: bert/encoder/layer_5/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_5/output/LayerNorm/beta, found name: bert/encoder/layer_5/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_6/attention/self/query/kernel, found name: bert/encoder/layer_6/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_6/attention/self/query/bias, found name: bert/encoder/layer_6/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_6/attention/self/key/kernel, found name: bert/encoder/layer_6/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_6/attention/self/key/bias, found name: bert/encoder/layer_6/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_6/attention/self/value/kernel, found name: bert/encoder/layer_6/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_6/attention/self/value/bias, found name: bert/encoder/layer_6/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_6/attention/output/dense/kernel, found name: bert/encoder/layer_6/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_6/attention/output/dense/bias, found name: bert/encoder/layer_6/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_6/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_6/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_6/attention/output/LayerNorm/beta, found name: bert/encoder/layer_6/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_6/intermediate/dense/kernel, found name: bert/encoder/layer_6/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_6/intermediate/dense/bias, found name: bert/encoder/layer_6/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_6/output/dense/kernel, found name: bert/encoder/layer_6/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_6/output/dense/bias, found name: bert/encoder/layer_6/output/dense/bias\n",
            "==> searching: bert/encoder/layer_6/output/LayerNorm/gamma, found name: bert/encoder/layer_6/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_6/output/LayerNorm/beta, found name: bert/encoder/layer_6/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_7/attention/self/query/kernel, found name: bert/encoder/layer_7/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_7/attention/self/query/bias, found name: bert/encoder/layer_7/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_7/attention/self/key/kernel, found name: bert/encoder/layer_7/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_7/attention/self/key/bias, found name: bert/encoder/layer_7/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_7/attention/self/value/kernel, found name: bert/encoder/layer_7/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_7/attention/self/value/bias, found name: bert/encoder/layer_7/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_7/attention/output/dense/kernel, found name: bert/encoder/layer_7/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_7/attention/output/dense/bias, found name: bert/encoder/layer_7/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_7/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_7/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_7/attention/output/LayerNorm/beta, found name: bert/encoder/layer_7/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_7/intermediate/dense/kernel, found name: bert/encoder/layer_7/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_7/intermediate/dense/bias, found name: bert/encoder/layer_7/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_7/output/dense/kernel, found name: bert/encoder/layer_7/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_7/output/dense/bias, found name: bert/encoder/layer_7/output/dense/bias\n",
            "==> searching: bert/encoder/layer_7/output/LayerNorm/gamma, found name: bert/encoder/layer_7/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_7/output/LayerNorm/beta, found name: bert/encoder/layer_7/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_8/attention/self/query/kernel, found name: bert/encoder/layer_8/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_8/attention/self/query/bias, found name: bert/encoder/layer_8/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_8/attention/self/key/kernel, found name: bert/encoder/layer_8/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_8/attention/self/key/bias, found name: bert/encoder/layer_8/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_8/attention/self/value/kernel, found name: bert/encoder/layer_8/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_8/attention/self/value/bias, found name: bert/encoder/layer_8/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_8/attention/output/dense/kernel, found name: bert/encoder/layer_8/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_8/attention/output/dense/bias, found name: bert/encoder/layer_8/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_8/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_8/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_8/attention/output/LayerNorm/beta, found name: bert/encoder/layer_8/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_8/intermediate/dense/kernel, found name: bert/encoder/layer_8/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_8/intermediate/dense/bias, found name: bert/encoder/layer_8/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_8/output/dense/kernel, found name: bert/encoder/layer_8/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_8/output/dense/bias, found name: bert/encoder/layer_8/output/dense/bias\n",
            "==> searching: bert/encoder/layer_8/output/LayerNorm/gamma, found name: bert/encoder/layer_8/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_8/output/LayerNorm/beta, found name: bert/encoder/layer_8/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_9/attention/self/query/kernel, found name: bert/encoder/layer_9/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_9/attention/self/query/bias, found name: bert/encoder/layer_9/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_9/attention/self/key/kernel, found name: bert/encoder/layer_9/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_9/attention/self/key/bias, found name: bert/encoder/layer_9/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_9/attention/self/value/kernel, found name: bert/encoder/layer_9/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_9/attention/self/value/bias, found name: bert/encoder/layer_9/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_9/attention/output/dense/kernel, found name: bert/encoder/layer_9/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_9/attention/output/dense/bias, found name: bert/encoder/layer_9/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_9/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_9/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_9/attention/output/LayerNorm/beta, found name: bert/encoder/layer_9/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_9/intermediate/dense/kernel, found name: bert/encoder/layer_9/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_9/intermediate/dense/bias, found name: bert/encoder/layer_9/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_9/output/dense/kernel, found name: bert/encoder/layer_9/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_9/output/dense/bias, found name: bert/encoder/layer_9/output/dense/bias\n",
            "==> searching: bert/encoder/layer_9/output/LayerNorm/gamma, found name: bert/encoder/layer_9/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_9/output/LayerNorm/beta, found name: bert/encoder/layer_9/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_10/attention/self/query/kernel, found name: bert/encoder/layer_10/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_10/attention/self/query/bias, found name: bert/encoder/layer_10/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_10/attention/self/key/kernel, found name: bert/encoder/layer_10/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_10/attention/self/key/bias, found name: bert/encoder/layer_10/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_10/attention/self/value/kernel, found name: bert/encoder/layer_10/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_10/attention/self/value/bias, found name: bert/encoder/layer_10/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_10/attention/output/dense/kernel, found name: bert/encoder/layer_10/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_10/attention/output/dense/bias, found name: bert/encoder/layer_10/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_10/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_10/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_10/attention/output/LayerNorm/beta, found name: bert/encoder/layer_10/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_10/intermediate/dense/kernel, found name: bert/encoder/layer_10/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_10/intermediate/dense/bias, found name: bert/encoder/layer_10/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_10/output/dense/kernel, found name: bert/encoder/layer_10/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_10/output/dense/bias, found name: bert/encoder/layer_10/output/dense/bias\n",
            "==> searching: bert/encoder/layer_10/output/LayerNorm/gamma, found name: bert/encoder/layer_10/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_10/output/LayerNorm/beta, found name: bert/encoder/layer_10/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_11/attention/self/query/kernel, found name: bert/encoder/layer_11/attention/self/query/kernel\n",
            "==> searching: bert/encoder/layer_11/attention/self/query/bias, found name: bert/encoder/layer_11/attention/self/query/bias\n",
            "==> searching: bert/encoder/layer_11/attention/self/key/kernel, found name: bert/encoder/layer_11/attention/self/key/kernel\n",
            "==> searching: bert/encoder/layer_11/attention/self/key/bias, found name: bert/encoder/layer_11/attention/self/key/bias\n",
            "==> searching: bert/encoder/layer_11/attention/self/value/kernel, found name: bert/encoder/layer_11/attention/self/value/kernel\n",
            "==> searching: bert/encoder/layer_11/attention/self/value/bias, found name: bert/encoder/layer_11/attention/self/value/bias\n",
            "==> searching: bert/encoder/layer_11/attention/output/dense/kernel, found name: bert/encoder/layer_11/attention/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_11/attention/output/dense/bias, found name: bert/encoder/layer_11/attention/output/dense/bias\n",
            "==> searching: bert/encoder/layer_11/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_11/attention/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_11/attention/output/LayerNorm/beta, found name: bert/encoder/layer_11/attention/output/LayerNorm/beta\n",
            "==> searching: bert/encoder/layer_11/intermediate/dense/kernel, found name: bert/encoder/layer_11/intermediate/dense/kernel\n",
            "==> searching: bert/encoder/layer_11/intermediate/dense/bias, found name: bert/encoder/layer_11/intermediate/dense/bias\n",
            "==> searching: bert/encoder/layer_11/output/dense/kernel, found name: bert/encoder/layer_11/output/dense/kernel\n",
            "==> searching: bert/encoder/layer_11/output/dense/bias, found name: bert/encoder/layer_11/output/dense/bias\n",
            "==> searching: bert/encoder/layer_11/output/LayerNorm/gamma, found name: bert/encoder/layer_11/output/LayerNorm/gamma\n",
            "==> searching: bert/encoder/layer_11/output/LayerNorm/beta, found name: bert/encoder/layer_11/output/LayerNorm/beta\n",
            "==> searching: bert/pooler/dense/kernel, found name: bert/pooler/dense/kernel\n",
            "==> searching: bert/pooler/dense/bias, found name: bert/pooler/dense/bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIHpFu5iYKaC",
        "colab_type": "code",
        "outputId": "e37605c4-e15f-4490-b9e4-39560a92bcea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Sequence-Mask (ZeroMasking)     (None, None)         0           Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (Embedding)     (None, None, 768)    16226304    Sequence-Mask[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, None, 768)    0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, None, 768)    0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, None, 768)    0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, None, 768)    0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, None, 768)    0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, None, 768)    0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, None, 768)    0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, None, 768)    0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, None, 768)    0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, None, 768)    0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, None, 768)    1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, None, 768)    0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, None, 768)    1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, None, 768)    0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, None, 768)    1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Pooler (Lambda)                 (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Pooler-Dense (Dense)            (None, 768)          590592      Pooler[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 768)          0           Pooler-Dense[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_146 (Dense)               (None, 3)            2307        dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 102,269,955\n",
            "Trainable params: 102,269,955\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kxmPNhEPJbM",
        "colab_type": "code",
        "outputId": "bc151f3b-e800-4f64-9ce7-3bfaa52b2a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "filepath = \"/content/drive/My Drive/model/weights-improvement-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}.h5\"\n",
        "cp_callback = ModelCheckpoint(\n",
        "    filepath=filepath, verbose=1, save_best_only=True,\n",
        ")\n",
        "save_check_point = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "callbacks_list = [cp_callback, save_check_point]\n",
        "model.fit_generator(\n",
        "    train,\n",
        "    validation_data=test,\n",
        "    validation_steps=len(x_test)/ 4,\n",
        "    steps_per_epoch=len(x_train) / batch,\n",
        "    verbose=1,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks_list,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "11/10 [================================] - 26s 2s/step - loss: 0.9787 - acc: 0.5714 - val_loss: 0.9474 - val_acc: 0.5263\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.94736, saving model to /content/drive/My Drive/model/weights-improvement-01-0.95-0.53.h5\n",
            "Epoch 2/10\n",
            "11/10 [================================] - 10s 938ms/step - loss: 0.7510 - acc: 0.6572 - val_loss: 0.4077 - val_acc: 0.9474\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.94736 to 0.40766, saving model to /content/drive/My Drive/model/weights-improvement-02-0.41-0.95.h5\n",
            "Epoch 3/10\n",
            "11/10 [================================] - 10s 953ms/step - loss: 0.5963 - acc: 0.7714 - val_loss: 0.6651 - val_acc: 0.6842\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.40766\n",
            "Epoch 4/10\n",
            "11/10 [================================] - 11s 963ms/step - loss: 0.4689 - acc: 0.8343 - val_loss: 0.4831 - val_acc: 0.7895\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.40766\n",
            "Epoch 5/10\n",
            "11/10 [================================] - 10s 948ms/step - loss: 0.3839 - acc: 0.8914 - val_loss: 0.3824 - val_acc: 0.9474\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.40766 to 0.38243, saving model to /content/drive/My Drive/model/weights-improvement-05-0.38-0.95.h5\n",
            "Epoch 6/10\n",
            "11/10 [================================] - 10s 932ms/step - loss: 0.3025 - acc: 0.9029 - val_loss: 0.5456 - val_acc: 0.6842\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.38243\n",
            "Epoch 7/10\n",
            "11/10 [================================] - 10s 926ms/step - loss: 0.2202 - acc: 0.9543 - val_loss: 0.2106 - val_acc: 0.8947\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.38243 to 0.21064, saving model to /content/drive/My Drive/model/weights-improvement-07-0.21-0.89.h5\n",
            "Epoch 8/10\n",
            "11/10 [================================] - 10s 930ms/step - loss: 0.1296 - acc: 0.9829 - val_loss: 0.4455 - val_acc: 0.8421\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.21064\n",
            "Epoch 9/10\n",
            "11/10 [================================] - 10s 939ms/step - loss: 0.0927 - acc: 0.9771 - val_loss: 0.2879 - val_acc: 0.9474\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.21064\n",
            "Epoch 10/10\n",
            "11/10 [================================] - 11s 956ms/step - loss: 0.0797 - acc: 0.9886 - val_loss: 0.4409 - val_acc: 0.7895\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.21064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcd883664a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSf-CfssLeNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "new_model = load_model('/content/drive/My Drive/model/weights-improvement-03-0.14-1.0.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnarDdmL3RD7",
        "colab_type": "code",
        "outputId": "4882e9be-8a38-4820-edc0-aafa454deb06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "new_model.evaluate_generator(test, 4, verbose=1,)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 90ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.054578081766764326, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtdSZ0qQShlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = '''美陸雙方15日正式簽署第一階段貿易協議，雙方貿易戰暫時休兵。但分析師認為，這項協議僅結束全球經濟的部分不確定性，但美國與其貿易夥伴的緊張關係恐將持續，預計川普政府下個目標將是對歐洲提高關稅。\n",
        "\n",
        "外媒報導，針對美陸簽署第一階段貿易協議，Strategas政策研究主管Daniel Clifton認為，美國與大陸未來仍將維持緊張局勢，包括網路、國家安全和人權方面，「這些問題並不會消失，但對標普500指數成分股的獲利影響沒有關稅升級來的大」。\n",
        "\n",
        "花旗集團經濟學家Cesar Rojas也預期，美陸貿易緊張局勢今年會持續上演。他表示，美方在第一階段協議中利用關稅向大陸施壓，但現在可能會改為對陸投資進行更嚴格的監管審查。\n",
        "\n",
        "報導指出，世界貿易組織先前裁決空巴長期獲得政府非法補貼，讓美國揚言開徵報復性關稅；加上法國要求谷歌、蘋果等美企繳交數位稅，美方也考慮對法國葡萄酒等商品徵收關稅，預計川普對歐洲加徵關稅成為新風險。\n",
        "\n",
        "美東時間15日，美國總統川普與大陸國務院副總理劉鶴正式簽署首階段貿易協議，本次協議主要內容包含陸方將在未來兩年採購超過2000億美元的美國產品，同時陸方將開放金融市場，停止強迫美方向陸方轉讓技術。'''\n",
        "x1,x2 = tokenize.encode(first_text=text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY3AO_SPJdhq",
        "colab_type": "code",
        "outputId": "5dc44bff-bd6e-422f-f8c7-e8d44e7f6f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "new_model.predict([x1,x2], verbose=1).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "472/472 [==============================] - 0s 370us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(472, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YORWGM5Hn2zy",
        "colab_type": "code",
        "outputId": "a1f96449-3886-424f-d8b4-959e9c2c598e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array([np.array(x1).reshape(1, -1), np.array(x2).reshape(1, -1)]).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1, 399)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBXhGNiFoNXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a, b = np.array(x1).reshape(1, -1), np.array(x2).reshape(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYcNBMEczVi_",
        "colab_type": "code",
        "outputId": "a53e44a9-ec10-4044-db02-8377b36dd302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.argmax(new_model.predict([a,b])[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM9hmLSe0cT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}